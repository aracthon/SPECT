{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-6941b35f7b97>:80 in main.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized!\n",
      "\n",
      "Training.\n",
      "0 loss:  210250.0\n",
      "1 loss:  775002.0\n",
      "\n",
      "2 loss:  56702.0\n",
      "3 loss:  19347.6\n",
      "\n",
      "4 loss:  222611.0\n",
      "5 loss:  194526.0\n",
      "\n",
      "6 loss:  194106.0\n",
      "7 loss:  77008.7\n",
      "\n",
      "8 loss:  147754.0\n",
      "9 loss:  40617.6\n",
      "\n",
      "10 loss:  161290.0\n",
      "11 loss:  80340.7\n",
      "\n",
      "12 loss:  138429.0\n",
      "13 loss:  40072.5\n",
      "\n",
      "14 loss:  191550.0\n",
      "15 loss:  129093.0\n",
      "\n",
      "16 loss:  186605.0\n",
      "17 loss:  60584.3\n",
      "\n",
      "18 loss:  138690.0\n",
      "19 loss:  45814.0\n",
      "\n",
      "20 loss:  170938.0\n",
      "21 loss:  105127.0\n",
      "\n",
      "22 loss:  138866.0\n",
      "23 loss:  36693.4\n",
      "\n",
      "24 loss:  185447.0\n",
      "25 loss:  125415.0\n",
      "\n",
      "26 loss:  177783.0\n",
      "27 loss:  50084.8\n",
      "\n",
      "28 loss:  155585.0\n",
      "29 loss:  57098.3\n",
      "\n",
      "30 loss:  163228.0\n",
      "31 loss:  69478.0\n",
      "\n",
      "32 loss:  139335.0\n",
      "33 loss:  44673.7\n",
      "\n",
      "34 loss:  150418.0\n",
      "35 loss:  73166.0\n",
      "\n",
      "36 loss:  125737.0\n",
      "37 loss:  42294.9\n",
      "\n",
      "38 loss:  200991.0\n",
      "39 loss:  145250.0\n",
      "\n",
      "40 loss:  184766.0\n",
      "41 loss:  57875.8\n",
      "\n",
      "42 loss:  137211.0\n",
      "43 loss:  45461.8\n",
      "\n",
      "44 loss:  158134.0\n",
      "45 loss:  82262.8\n",
      "\n",
      "46 loss:  130897.0\n",
      "47 loss:  40387.6\n",
      "\n",
      "48 loss:  202094.0\n",
      "49 loss:  154746.0\n",
      "\n",
      "50 loss:  188616.0\n",
      "51 loss:  54452.3\n",
      "\n",
      "52 loss:  130149.0\n",
      "53 loss:  43239.1\n",
      "\n",
      "54 loss:  158296.0\n",
      "55 loss:  92288.2\n",
      "\n",
      "56 loss:  127888.0\n",
      "57 loss:  37082.1\n",
      "\n",
      "58 loss:  198868.0\n",
      "59 loss:  151365.0\n",
      "\n",
      "60 loss:  182390.0\n",
      "61 loss:  53658.2\n",
      "\n",
      "62 loss:  129586.0\n",
      "63 loss:  44906.3\n",
      "\n",
      "64 loss:  155354.0\n",
      "65 loss:  86580.2\n",
      "\n",
      "66 loss:  128020.0\n",
      "67 loss:  38137.3\n",
      "\n",
      "68 loss:  195950.0\n",
      "69 loss:  146820.0\n",
      "\n",
      "70 loss:  179182.0\n",
      "71 loss:  55588.0\n",
      "\n",
      "72 loss:  130711.0\n",
      "73 loss:  43978.4\n",
      "\n",
      "74 loss:  158321.0\n",
      "75 loss:  92466.6\n",
      "\n",
      "76 loss:  128837.0\n",
      "77 loss:  35496.4\n",
      "\n",
      "78 loss:  205292.0\n",
      "79 loss:  162406.0\n",
      "\n",
      "80 loss:  189632.0\n",
      "81 loss:  56660.0\n",
      "\n",
      "82 loss:  127079.0\n",
      "83 loss:  41419.7\n",
      "\n",
      "84 loss:  193459.0\n",
      "85 loss:  136303.0\n",
      "\n",
      "86 loss:  181941.0\n",
      "87 loss:  56067.8\n",
      "\n",
      "88 loss:  156353.0\n",
      "89 loss:  62250.3\n",
      "\n",
      "90 loss:  145881.0\n",
      "91 loss:  48262.7\n",
      "\n",
      "92 loss:  165846.0\n",
      "93 loss:  90871.0\n",
      "\n",
      "94 loss:  138361.0\n",
      "95 loss:  44236.5\n",
      "\n",
      "96 loss:  208418.0\n",
      "97 loss:  155704.0\n",
      "\n",
      "98 loss:  194411.0\n",
      "99 loss:  62406.4\n",
      "\n",
      "100 loss:  144802.0\n",
      "101 loss:  46693.9\n",
      "\n",
      "102 loss:  163430.0\n",
      "103 loss:  85885.2\n",
      "\n",
      "104 loss:  137145.0\n",
      "105 loss:  41217.6\n",
      "\n",
      "106 loss:  204808.0\n",
      "107 loss:  156537.0\n",
      "\n",
      "108 loss:  191282.0\n",
      "109 loss:  61360.2\n",
      "\n",
      "110 loss:  132579.0\n",
      "111 loss:  42934.5\n",
      "\n",
      "112 loss:  164364.0\n",
      "113 loss:  101181.0\n",
      "\n",
      "114 loss:  131513.0\n",
      "115 loss:  35856.4\n",
      "\n",
      "116 loss:  197887.0\n",
      "117 loss:  148480.0\n",
      "\n",
      "118 loss:  182631.0\n",
      "119 loss:  55344.4\n",
      "\n",
      "120 loss:  136853.0\n",
      "121 loss:  47480.2\n",
      "\n",
      "122 loss:  167671.0\n",
      "123 loss:  92368.5\n",
      "\n",
      "124 loss:  141759.0\n",
      "125 loss:  42250.7\n",
      "\n",
      "126 loss:  167456.0\n",
      "127 loss:  116038.0\n",
      "\n",
      "128 loss:  131475.0\n",
      "129 loss:  37010.4\n",
      "\n",
      "130 loss:  178327.0\n",
      "131 loss:  115212.0\n",
      "\n",
      "132 loss:  177824.0\n",
      "133 loss:  48498.3\n",
      "\n",
      "134 loss:  153635.0\n",
      "135 loss:  55806.0\n",
      "\n",
      "136 loss:  161696.0\n",
      "137 loss:  68535.0\n",
      "\n",
      "138 loss:  138338.0\n",
      "139 loss:  44110.3\n",
      "\n",
      "140 loss:  159797.0\n",
      "141 loss:  96102.0\n",
      "\n",
      "142 loss:  129617.0\n",
      "143 loss:  35562.1\n",
      "\n",
      "144 loss:  195595.0\n",
      "145 loss:  145159.0\n",
      "\n",
      "146 loss:  180771.0\n",
      "147 loss:  58170.4\n",
      "\n",
      "148 loss:  129014.0\n",
      "149 loss:  43093.3\n",
      "\n",
      "150 loss:  205384.0\n",
      "151 loss:  153620.0\n",
      "\n",
      "152 loss:  186436.0\n",
      "153 loss:  62223.7\n",
      "\n",
      "154 loss:  139297.0\n",
      "155 loss:  46165.3\n",
      "\n",
      "156 loss:  156144.0\n",
      "157 loss:  74800.1\n",
      "\n",
      "158 loss:  133109.0\n",
      "159 loss:  40943.3\n",
      "\n",
      "160 loss:  200848.0\n",
      "161 loss:  144011.0\n",
      "\n",
      "162 loss:  186296.0\n",
      "163 loss:  62960.6\n",
      "\n",
      "164 loss:  140146.0\n",
      "165 loss:  46994.6\n",
      "\n",
      "166 loss:  142716.0\n",
      "167 loss:  56629.7\n",
      "\n",
      "168 loss:  146295.0\n",
      "169 loss:  51829.5\n",
      "\n",
      "170 loss:  188591.0\n",
      "171 loss:  120518.0\n",
      "\n",
      "172 loss:  157796.0\n",
      "173 loss:  44986.5\n",
      "\n",
      "174 loss:  160044.0\n",
      "175 loss:  86085.7\n",
      "\n",
      "176 loss:  136492.0\n",
      "177 loss:  44452.4\n",
      "\n",
      "178 loss:  202210.0\n",
      "179 loss:  149368.0\n",
      "\n",
      "180 loss:  189280.0\n",
      "181 loss:  63086.4\n",
      "\n",
      "182 loss:  138065.0\n",
      "183 loss:  45268.6\n",
      "\n",
      "184 loss:  210896.0\n",
      "185 loss:  158076.0\n",
      "\n",
      "186 loss:  198963.0\n",
      "187 loss:  66401.1\n",
      "\n",
      "188 loss:  137363.0\n",
      "189 loss:  43113.1\n",
      "\n",
      "190 loss:  210159.0\n",
      "191 loss:  157252.0\n",
      "\n",
      "192 loss:  196799.0\n",
      "193 loss:  56978.7\n",
      "\n",
      "194 loss:  147754.0\n",
      "195 loss:  54642.4\n",
      "\n",
      "196 loss:  130080.0\n",
      "197 loss:  46728.0\n",
      "\n",
      "198 loss:  209118.0\n",
      "199 loss:  178210.0\n",
      "\n",
      "\n",
      "Weight matrix.\n",
      "[[ -12.57746887]\n",
      " [  43.35622025]\n",
      " [ -26.5865593 ]\n",
      " [  75.92259979]\n",
      " [ -42.03608704]\n",
      " [  59.08722687]\n",
      " [  89.93351746]\n",
      " [ 192.44241333]\n",
      " [  31.36868668]\n",
      " [  49.2036438 ]\n",
      " [ 155.93173218]\n",
      " [  17.9601059 ]\n",
      " [ 192.44155884]\n",
      " [  49.68681717]\n",
      " [  28.44672012]\n",
      " [ 122.10012817]\n",
      " [ 114.83415222]\n",
      " [  36.73336029]\n",
      " [  18.62533569]\n",
      " [  65.52030945]\n",
      " [  86.0497818 ]\n",
      " [  79.75686646]]\n",
      "\n",
      "Bias vector.\n",
      "[-300.]\n",
      "\n",
      "Applying model to first test instance.\n",
      "\n",
      "Accuracy on train: 0.85\n",
      "Accuracy on test: 0.748663\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Global variables.\n",
    "BATCH_SIZE = 40  # The number of training examples to use per training step.\n",
    "\n",
    "def extract_data(filename):\n",
    "\n",
    "    out = np.loadtxt(filename, delimiter=',');\n",
    "\n",
    "    # Arrays to hold the labels and feature vectors.\n",
    "    labels = out[:,0]\n",
    "    labels = labels.reshape(labels.size,1)\n",
    "    fvecs = out[:,1:]\n",
    "\n",
    "    # Return a pair of the feature matrix and the one-hot label matrix.\n",
    "    return fvecs,labels\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    # Be verbose?\n",
    "    verbose = True\n",
    "\n",
    "    # Plot? \n",
    "    plot = False\n",
    "    \n",
    "    # Get the data.\n",
    "    train_data_filename = '../data/SPECT.train.txt'\n",
    "    test_data_filename = '../data/SPECT.test.txt'\n",
    "\n",
    "    # Extract it into numpy matrices.\n",
    "    train_data, train_labels = extract_data(train_data_filename)\n",
    "    test_data, test_labels = extract_data(test_data_filename)\n",
    "\n",
    "    # Convert labels to +1,-1\n",
    "    train_labels[train_labels==0] = -1\n",
    "    test_labels[test_labels==0] = -1\n",
    "\n",
    "    # Get the shape of the training data.\n",
    "    train_size, num_features = train_data.shape\n",
    "    test_size, num_features = test_data.shape\n",
    "\n",
    "    # Get the number of epochs for training.\n",
    "    num_epochs = 100\n",
    "\n",
    "    # Get the C param of SVM\n",
    "    svmC = 100#FLAGS.svmC\n",
    "\n",
    "    # This is where training samples and labels are fed to the graph.\n",
    "    # These placeholder nodes will be fed a batch of training data at each\n",
    "    # training step using the {feed_dict} argument to the Run() call below.\n",
    "    x = tf.placeholder(\"float\", shape=[None, num_features])\n",
    "    y = tf.placeholder(\"float\", shape=[None,1])\n",
    "\n",
    "    # Define and initialize the network.\n",
    "\n",
    "    # These are the weights that inform how much each feature contributes to\n",
    "    # the classification.\n",
    "    W = tf.Variable(tf.zeros([num_features,1]))\n",
    "    b = tf.Variable(tf.zeros([1]))\n",
    "    y_raw = tf.matmul(x,W) + b\n",
    "\n",
    "    # Optimization.\n",
    "    regularization_loss = 0.5*tf.reduce_sum(tf.square(W)) \n",
    "    hinge_loss = tf.reduce_sum(tf.maximum(tf.zeros([BATCH_SIZE,1]), \n",
    "        1 - y*y_raw));\n",
    "    svm_loss = regularization_loss + svmC*hinge_loss;\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(svm_loss)\n",
    "\n",
    "    # Evaluation.\n",
    "    predicted_class = tf.sign(y_raw);\n",
    "    correct_prediction = tf.equal(y,predicted_class)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    # Create a local session to run this computation.\n",
    "    with tf.Session() as s:\n",
    "        # Run all the initializers to prepare the trainable parameters.\n",
    "        tf.initialize_all_variables().run()\n",
    "        if verbose:\n",
    "            print 'Initialized!'\n",
    "            print\n",
    "            print 'Training.'\n",
    "\n",
    "        # Iterate and train.\n",
    "        for step in xrange(num_epochs * train_size // BATCH_SIZE):\n",
    "            if verbose:\n",
    "                print step,\n",
    "\n",
    "            offset = (step * BATCH_SIZE) % train_size\n",
    "            batch_data = train_data[offset:(offset + BATCH_SIZE), :]\n",
    "            batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "            train_step.run(feed_dict={x: batch_data, y: batch_labels})\n",
    "            print 'loss: ', svm_loss.eval(feed_dict={x: batch_data, y: batch_labels})\n",
    "\n",
    "            if verbose and offset >= train_size-BATCH_SIZE:\n",
    "                print\n",
    "\n",
    "        # Give very detailed output.\n",
    "        if verbose:\n",
    "            print\n",
    "            print 'Weight matrix.'\n",
    "            print s.run(W)\n",
    "            print\n",
    "            print 'Bias vector.'\n",
    "            print s.run(b)\n",
    "            print\n",
    "            print \"Applying model to first test instance.\"\n",
    "            print\n",
    "        \n",
    "        print \"Accuracy on train:\", accuracy.eval(feed_dict={x: train_data, y: train_labels})\n",
    "        print \"Accuracy on test:\", accuracy.eval(feed_dict={x: test_data, y: test_labels})\n",
    "        \n",
    "        # test\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
